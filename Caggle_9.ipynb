{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Lenovo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-5-30 Python-3.9.19 torch-2.2.2 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoShape(\n",
       "  (model): DetectMultiBackend(\n",
       "    (model): DetectionModel(\n",
       "      (model): Sequential(\n",
       "        (0): Conv(\n",
       "          (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Conv(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (4): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Conv(\n",
       "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (6): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Conv(\n",
       "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (8): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): SPPF(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (10): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "        (12): Concat()\n",
       "        (13): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (15): Upsample(scale_factor=2.0, mode='nearest')\n",
       "        (16): Concat()\n",
       "        (17): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (18): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (19): Concat()\n",
       "        (20): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (21): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (22): Concat()\n",
       "        (23): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (24): Detect(\n",
       "          (m): ModuleList(\n",
       "            (0): Conv2d(128, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# device = 'cpu'\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load an image from a URL\n",
    "def load_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "\n",
    "# Function to perform detection\n",
    "def detect_objects(image, model):\n",
    "    # Directly pass the image to the model\n",
    "    with torch.no_grad():\n",
    "        results = model(image)\n",
    "\n",
    "    # Extract the bounding boxes and data\n",
    "    results = results.pandas().xyxy[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a frame to PIL Image\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Function to convert PIL Image back to frame\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://english.news.cn/europe/20230716/adc36d89de9f4699bdb80290a80df2b8/20230716adc36d89de9f4699bdb80290a80df2b8_2023071658a775383fd84927a756ff4aa59911c5.jpg'\n",
    "url = 'https://bearizona.com/wp-content/uploads/2022/10/GD3A0304-min.jpeg'\n",
    "\n",
    "raw_image = load_image(url)\n",
    "results = detect_objects(raw_image, model)\n",
    "results\n",
    "\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default(size=20)\n",
    "    \n",
    "    for index, row in results.iterrows():\n",
    "        xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
    "\n",
    "        # if xmin > xmax or ymin > ymax:\n",
    "        #     continue\n",
    "        \n",
    "        confidence = row['confidence']\n",
    "        label = row['name'] # bear 50%\n",
    "        \n",
    "        colors = {\n",
    "            'person': 'red', 'car': 'blue'\n",
    "        }\n",
    "        \n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline=colors.get(label, 'purple'), width=3)\n",
    "        \n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "\n",
    "        # Put text label on image\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Draw the bounding boxes on the image\n",
    "image = raw_image.copy()\n",
    "image_with_boxes = draw_boxes(image, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The FPS of the video is: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|██████████| 3766/3766 [10:10<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_yolo.mp4'\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the FPS (Frames Per Second) of the video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(f\"The FPS of the video is: {fps}\")\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "with tqdm(total=total_frames, desc=f\"Processing Video\") as pbar:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        image = frame_to_image(frame)\n",
    "        \n",
    "        # Perform detection\n",
    "        results = detect_objects(image, model)\n",
    "        \n",
    "        # Draw the bounding boxes\n",
    "        image_with_boxes = draw_boxes(image, results)\n",
    "        \n",
    "        # Convert back to frame\n",
    "        output_frame = image_to_frame(image_with_boxes)\n",
    "        \n",
    "        out.write(output_frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# только для людей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Lenovo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-5-30 Python-3.9.19 torch-2.2.2 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "Обработка видео: 100%|██████████| 3766/3766 [09:17<00:00,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка видео заняла: 557.95 секунд\n",
      "Кадров обработано в секунду: 6.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Загрузка модели YOLOv5\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Использование устройства MPS, если доступно, иначе CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people(image, model):\n",
    "    with torch.no_grad():\n",
    "        results = model(image)\n",
    "    # Фильтрация по метке 'person' (человек)\n",
    "    results = results.pandas().xyxy[0]\n",
    "    people = results[results['name'] == 'person']\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for _, row in results.iterrows():\n",
    "        xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
    "        confidence = row['confidence']\n",
    "        label = row['name']\n",
    "        \n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Обработка видео и сохранение результата\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_yolo.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=total_frames, desc=\"Обработка видео\") as pbar:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        image = frame_to_image(frame)\n",
    "        results = detect_people(image, model)\n",
    "        image_with_boxes = draw_boxes(image, results)\n",
    "        output_frame = image_to_frame(image_with_boxes)\n",
    "        out.write(output_frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "frames_per_second_processed = total_frames / elapsed_time\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"Обработка видео заняла: {elapsed_time:.2f} секунд\")\n",
    "print(f\"Кадров обработано в секунду: {frames_per_second_processed:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступен: True\n",
      "Количество GPU: 1\n",
      "Имя устройства: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA доступен:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Количество GPU:\", torch.cuda.device_count())\n",
    "    print(\"Имя устройства:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA недоступен. Проверьте драйверы и установку CUDA.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Обработка видео: 100%|██████████| 3766/3766 [07:52<00:00,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка видео заняла: 472.68 секунд\n",
      "Кадров обработано в секунду: 7.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "# Загрузка модели YOLOS-Tiny\n",
    "processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "\n",
    "# Использование устройства CUDA, если доступно, иначе CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people(image, model):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    \n",
    "    # Извлечение людей из результатов\n",
    "    people = []\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if label == 1:  # Assuming 'person' label is 1\n",
    "            people.append({'score': score.item(), 'box': box.tolist(), 'label': 'person'})\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for res in results:\n",
    "        xmin, ymin, xmax, ymax = res['box']\n",
    "        confidence = res['score']\n",
    "        label = res['label']\n",
    "        \n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Обработка видео и сохранение результата\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_yolo.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=total_frames, desc=\"Обработка видео\") as pbar:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        image = frame_to_image(frame)\n",
    "        results = detect_people(image, model)\n",
    "        image_with_boxes = draw_boxes(image, results)\n",
    "        output_frame = image_to_frame(image_with_boxes)\n",
    "        out.write(output_frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "frames_per_second_processed = total_frames / elapsed_time\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"Обработка видео заняла: {elapsed_time:.2f} секунд\")\n",
    "print(f\"Кадров обработано в секунду: {frames_per_second_processed:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка видео: 100%|██████████| 3766/3766 [04:18<00:00, 14.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка видео заняла: 258.10 секунд\n",
      "Кадров обработано в секунду: 14.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "# Загрузка модели YOLOS-Tiny\n",
    "processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "\n",
    "# Использование устройства CUDA, если доступно, иначе CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people(image, model):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    \n",
    "    # Извлечение людей из результатов\n",
    "    people = []\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if label == 1:  # Assuming 'person' label is 1\n",
    "            people.append({'score': score.item(), 'box': box.tolist(), 'label': 'person'})\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for res in results:\n",
    "        xmin, ymin, xmax, ymax = res['box']\n",
    "        confidence = res['score']\n",
    "        label = res['label']\n",
    "        \n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Обработка видео и сохранение результата\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_yolo.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=total_frames, desc=\"Обработка видео\") as pbar:\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Пропуск некоторых кадров\n",
    "        if i % 2 != 0:\n",
    "            out.write(frame)\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        image = frame_to_image(frame)\n",
    "        results = detect_people(image, model)\n",
    "        image_with_boxes = draw_boxes(image, results)\n",
    "        output_frame = image_to_frame(image_with_boxes)\n",
    "        out.write(output_frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "frames_per_second_processed = total_frames / elapsed_time\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"Обработка видео заняла: {elapsed_time:.2f} секунд\")\n",
    "print(f\"Кадров обработано в секунду: {frames_per_second_processed:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Yolo-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Обработка всего видео (многопоточность): 100%|██████████| 3766/3766 [20:19<00:00,  3.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "# Загрузка модели YOLOS-Tiny\n",
    "processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "\n",
    "# Использование устройства CUDA, если доступно, иначе CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people(image, processor, model, device):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    \n",
    "    # Извлечение людей из результатов\n",
    "    people = []\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if label == 1:  # Assuming 'person' label is 1\n",
    "            people.append({'score': score.item(), 'box': box.tolist(), 'label': 'person'})\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for res in results:\n",
    "        xmin, ymin, xmax, ymax = res['box']\n",
    "        confidence = res['score']\n",
    "        label = res['label']\n",
    "        \n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Функция для обработки кадра\n",
    "def process_frame(frame, processor, model, device):\n",
    "    image = frame_to_image(frame)\n",
    "    results = detect_people(image, processor, model, device)\n",
    "    image_with_boxes = draw_boxes(image, results)\n",
    "    output_frame = image_to_frame(image_with_boxes)\n",
    "    return output_frame\n",
    "\n",
    "def process_video_multithreaded(video_path, output_path, processor, model, device, max_workers=2):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        with tqdm(total=total_frames, desc=\"Обработка всего видео (многопоточность)\") as pbar:\n",
    "            for i in range(total_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                if i % 2 == 0:\n",
    "                    futures[executor.submit(process_frame, frame, processor, model, device)] = i\n",
    "                else:\n",
    "                    out.write(frame)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                output_frame = future.result()\n",
    "                out.write(output_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# Пути к видеофайлам\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path_multi = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_yolo_multithreaded.mp4'\n",
    "\n",
    "\n",
    "# Обработка видео с многопоточностью\n",
    "process_video_multithreaded(video_path, output_path_multi, processor, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Обработка всего видео (многопоточность):   0%|          | 0/3766 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "# Загрузка модели YOLOS-Tiny\n",
    "processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "\n",
    "# Использование устройства CUDA, если доступно, иначе CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people(image, processor, model, device):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    \n",
    "    # Извлечение людей из результатов\n",
    "    people = []\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if label == 1:  # Assuming 'person' label is 1\n",
    "            people.append({'score': score.item(), 'box': box.tolist(), 'label': 'person'})\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for res in results:\n",
    "        xmin, ymin, xmax, ymax = res['box']\n",
    "        confidence = res['score']\n",
    "        label = res['label']\n",
    "        \n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Функция для обработки кадра\n",
    "def process_frame(frame, processor, model, device):\n",
    "    image = frame_to_image(frame)\n",
    "    results = detect_people(image, processor, model, device)\n",
    "    image_with_boxes = draw_boxes(image, results)\n",
    "    output_frame = image_to_frame(image_with_boxes)\n",
    "    return output_frame\n",
    "\n",
    "# Функция для загрузки видео в оперативную память\n",
    "def load_video_to_memory(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def process_video_multithreaded(frames, output_path, processor, model, device, max_workers=2):\n",
    "    fps = 30  # Установите здесь известное значение fps вашего видео\n",
    "    frame_height, frame_width = frames[0].shape[:2]\n",
    "    \n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        with tqdm(total=len(frames), desc=\"Обработка всего видео (многопоточность)\") as pbar:\n",
    "            # for i, frame in enumerate(frames):\n",
    "            #     # if i % 1 == 0:\n",
    "            #         futures[executor.submit(process_frame, frame, processor, model, device)] = i\n",
    "            #     else:\n",
    "            #         out.write(frame)\n",
    "            #     pbar.update(1)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                output_frame = future.result()\n",
    "                out.write(output_frame)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "# Пути к видеофайлам\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path_multi = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_yolo_multithreaded.mp4'\n",
    "\n",
    "# Загрузка видео в оперативную память\n",
    "frames = load_video_to_memory(video_path)\n",
    "\n",
    "# Обработка видео с многопоточностью\n",
    "process_video_multithreaded(frames, output_path_multi, processor, model, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo5s FinalVers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Lenovo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "YOLOv5  2024-5-30 Python-3.9.19 torch-2.2.2 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "Обработка всего видео (многопоточность): 100%|██████████| 3766/3766 [02:44<00:00, 22.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import torch\n",
    "\n",
    "# Загрузка модели YOLOv5\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people(frame, model, device):\n",
    "    results = model(frame)\n",
    "    detections = results.xyxy[0].cpu().numpy()\n",
    "\n",
    "    people = []\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, confidence, class_id = detection\n",
    "        if int(class_id) == 0:  # Assuming 'person' label is 0\n",
    "            people.append({'score': confidence, 'box': [x1, y1, x2, y2], 'label': 'person'})\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for res in results:\n",
    "        x1, y1, x2, y2 = res['box']\n",
    "        confidence = res['score']\n",
    "        label = res['label']\n",
    "\n",
    "        draw.rectangle([x1, y1, x2, y2], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((x1, y1), label_with_conf, fill='white', font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Функция для обработки кадра\n",
    "def process_frame(frame, model, device):\n",
    "    image = frame_to_image(frame)\n",
    "    results = detect_people(frame, model, device)\n",
    "    image_with_boxes = draw_boxes(image, results)\n",
    "    output_frame = image_to_frame(image_with_boxes)\n",
    "    return output_frame\n",
    "\n",
    "# Функция для загрузки видео в оперативную память\n",
    "def load_video_to_memory(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def process_video_multithreaded(frames, output_path, model, device, fps, max_workers=8):\n",
    "    frame_height, frame_width = frames[0].shape[:2]\n",
    "\n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        with tqdm(total=len(frames), desc=\"Обработка всего видео (многопоточность)\") as pbar:\n",
    "            for i, frame in enumerate(frames):\n",
    "                if i % 2 == 0:\n",
    "                    futures[executor.submit(process_frame, frame, model, device)] = i\n",
    "                else:\n",
    "                    out.write(frame)\n",
    "                pbar.update(1)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                output_frame = future.result()\n",
    "                out.write(output_frame)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "# Пути к видеофайлам\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path_multi = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_yolo_multithreaded.mp4'\n",
    "\n",
    "# Загрузка видео в оперативную память\n",
    "frames = load_video_to_memory(video_path)\n",
    "\n",
    "# Получение FPS видео\n",
    "fps = 30  # Установите здесь известное значение fps вашего видео или получите его из видеофайла\n",
    "\n",
    "# Обработка видео с многопоточностью\n",
    "process_video_multithreaded(frames, output_path_multi, model, device, fps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      | 352011 KiB |   1322 MiB |  17161 GiB |  17161 GiB |\\n|       from large pool | 325743 KiB |   1293 MiB |  17104 GiB |  17103 GiB |\\n|       from small pool |  26267 KiB |     49 MiB |     57 GiB |     57 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         | 352011 KiB |   1322 MiB |  17161 GiB |  17161 GiB |\\n|       from large pool | 325743 KiB |   1293 MiB |  17104 GiB |  17103 GiB |\\n|       from small pool |  26267 KiB |     49 MiB |     57 GiB |     57 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      | 350606 KiB |   1317 MiB |  16976 GiB |  16975 GiB |\\n|       from large pool | 324352 KiB |   1290 MiB |  16919 GiB |  16918 GiB |\\n|       from small pool |  26254 KiB |     49 MiB |     56 GiB |     56 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   | 444416 KiB |   1758 MiB |   1758 MiB |   1324 MiB |\\n|       from large pool | 401408 KiB |   1708 MiB |   1708 MiB |   1316 MiB |\\n|       from small pool |  43008 KiB |     50 MiB |     50 MiB |      8 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  92405 KiB | 232125 KiB |  11555 GiB |  11555 GiB |\\n|       from large pool |  75664 KiB | 219497 KiB |  11492 GiB |  11492 GiB |\\n|       from small pool |  16740 KiB |  17889 KiB |     63 GiB |     62 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     251    |     584    |    3179 K  |    3179 K  |\\n|       from large pool |      21    |     101    |    2030 K  |    2030 K  |\\n|       from small pool |     230    |     568    |    1149 K  |    1148 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     251    |     584    |    3179 K  |    3179 K  |\\n|       from large pool |      21    |     101    |    2030 K  |    2030 K  |\\n|       from small pool |     230    |     568    |    1149 K  |    1148 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      28    |      54    |      54    |      26    |\\n|       from large pool |       7    |      29    |      29    |      22    |\\n|       from small pool |      21    |      25    |      25    |       4    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      34    |      65    |    1502 K  |    1502 K  |\\n|       from large pool |       5    |      25    |    1089 K  |    1089 K  |\\n|       from small pool |      29    |      60    |     413 K  |     413 K  |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка всего видео (многопоточность): 100%|██████████| 3766/3766 [01:14<00:00, 141.15it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Загрузка модели Faster R-CNN\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people(image, model, device):\n",
    "    # Преобразование изображения\n",
    "    img = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Предсказание\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "    \n",
    "    # Обработка результатов\n",
    "    results = outputs[0]\n",
    "    people = []\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if label == 1 and score > 0.9:  # Assuming 'person' label is 1\n",
    "            people.append({'score': score.item(), 'box': box.tolist(), 'label': 'person'})\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for res in results:\n",
    "        xmin, ymin, xmax, ymax = res['box']\n",
    "        confidence = res['score']\n",
    "        label = res['label']\n",
    "        \n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Функция для обработки кадра\n",
    "def process_frame(frame, model, device):\n",
    "    image = frame_to_image(frame)\n",
    "    results = detect_people(image, model, device)\n",
    "    image_with_boxes = draw_boxes(image, results)\n",
    "    output_frame = image_to_frame(image_with_boxes)\n",
    "    return output_frame\n",
    "\n",
    "# Функция для загрузки видео в оперативную память\n",
    "def load_video_to_memory(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def process_video_multithreaded(frames, output_path, model, device, fps=30, max_workers=8, process_every_nth_frame=2):\n",
    "    frame_height, frame_width = frames[0].shape[:2]\n",
    "    \n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        with tqdm(total=len(frames), desc=\"Обработка всего видео (многопоточность)\") as pbar:\n",
    "            for i, frame in enumerate(frames):\n",
    "                if i % process_every_nth_frame == 0:\n",
    "                    futures[executor.submit(process_frame, frame, model, device)] = i\n",
    "                else:\n",
    "                    out.write(frame)\n",
    "                pbar.update(1)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    output_frame = future.result()\n",
    "                    out.write(output_frame)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при обработке кадра: {e}\")\n",
    "\n",
    "    out.release()\n",
    "\n",
    "# Пути к видеофайлам\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path_multi = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_faster_rcnn_multithreaded.mp4'\n",
    "\n",
    "# Загрузка видео в оперативную память\n",
    "frames = load_video_to_memory(video_path)\n",
    "\n",
    "# Обработка видео с многопоточностью\n",
    "process_video_multithreaded(frames, output_path_multi, model, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\langchain\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Обработка всего видео (многопоточность): 100%|██████████| 3766/3766 [04:59<00:00, 12.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "# Загрузка модели DETR\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people(image, processor, model, device):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Предсказание\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Обработка результатов\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    \n",
    "    # Извлечение людей из результатов\n",
    "    people = []\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if label == 1:  # Assuming 'person' label is 1\n",
    "            people.append({'score': score.item(), 'box': box.tolist(), 'label': 'person'})\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for res in results:\n",
    "        xmin, ymin, xmax, ymax = res['box']\n",
    "        confidence = res['score']\n",
    "        label = res['label']\n",
    "        \n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Функция для обработки кадра\n",
    "def process_frame(frame, processor, model, device):\n",
    "    image = frame_to_image(frame)\n",
    "    results = detect_people(image, processor, model, device)\n",
    "    image_with_boxes = draw_boxes(image, results)\n",
    "    output_frame = image_to_frame(image_with_boxes)\n",
    "    return output_frame\n",
    "\n",
    "# Функция для загрузки видео в оперативную память\n",
    "def load_video_to_memory(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def process_video_multithreaded(frames, output_path, processor, model, device, fps=30, max_workers=8, process_every_nth_frame=2):\n",
    "    frame_height, frame_width = frames[0].shape[:2]\n",
    "    \n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        with tqdm(total=len(frames), desc=\"Обработка всего видео (многопоточность)\") as pbar:\n",
    "            for i, frame in enumerate(frames):\n",
    "                if i % process_every_nth_frame == 0:\n",
    "                    futures[executor.submit(process_frame, frame, processor, model, device)] = i\n",
    "                else:\n",
    "                    out.write(frame)\n",
    "                pbar.update(1)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    output_frame = future.result()\n",
    "                    out.write(output_frame)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при обработке кадра: {e}\")\n",
    "\n",
    "    out.release()\n",
    "\n",
    "# Пути к видеофайлам\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path_multi = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_detr_multithreaded.mp4'\n",
    "\n",
    "# Загрузка видео в оперативную память\n",
    "frames = load_video_to_memory(video_path)\n",
    "\n",
    "# Обработка видео с многопоточностью\n",
    "process_video_multithreaded(frames, output_path_multi, processor, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка всего видео (многопоточность): 100%|██████████| 3766/3766 [04:56<00:00, 12.69it/s] \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Очистка кеша CUDA\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Загрузка модели MobileNetV3\n",
    "weights = SSDLite320_MobileNet_V3_Large_Weights.COCO_V1\n",
    "model = ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people(image, model, device):\n",
    "    # Преобразование изображения\n",
    "    img = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Предсказание\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "    \n",
    "    # Обработка результатов\n",
    "    results = outputs[0]\n",
    "    people = []\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if label == 1 and score > 0.9:  # Assuming 'person' label is 1\n",
    "            people.append({'score': score.item(), 'box': box.tolist(), 'label': 'person'})\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    for res in results:\n",
    "        xmin, ymin, xmax, ymax = res['box']\n",
    "        confidence = res['score']\n",
    "        label = res['label']\n",
    "        \n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Функция для обработки кадра\n",
    "def process_frame(frame, model, device):\n",
    "    image = frame_to_image(frame)\n",
    "    results = detect_people(image, model, device)\n",
    "    image_with_boxes = draw_boxes(image, results)\n",
    "    output_frame = image_to_frame(image_with_boxes)\n",
    "    return output_frame\n",
    "\n",
    "# Функция для загрузки видео в оперативную память\n",
    "def load_video_to_memory(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def process_video_multithreaded(frames, output_path, model, device, fps=30, max_workers=8, process_every_nth_frame=2):\n",
    "    frame_height, frame_width = frames[0].shape[:2]\n",
    "    \n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        with tqdm(total=len(frames), desc=\"Обработка всего видео (многопоточность)\") as pbar:\n",
    "            for i, frame in enumerate(frames):\n",
    "                if i % process_every_nth_frame == 0:\n",
    "                    futures[executor.submit(process_frame, frame, model, device)] = i\n",
    "                else:\n",
    "                    out.write(frame)\n",
    "                pbar.update(1)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    output_frame = future.result()\n",
    "                    out.write(output_frame)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при обработке кадра: {e}\")\n",
    "\n",
    "    out.release()\n",
    "\n",
    "# Пути к видеофайлам\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path_multi = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_mobilenet_multithreaded.mp4'\n",
    "\n",
    "# Загрузка видео в оперативную память\n",
    "frames = load_video_to_memory(video_path)\n",
    "\n",
    "# Обработка видео с многопоточностью\n",
    "process_video_multithreaded(frames, output_path_multi, model, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка всего видео (многопоточность): 100%|██████████| 3766/3766 [05:44<00:00, 10.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Загрузка модели RetinaNet\n",
    "weights = RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "model = retinanet_resnet50_fpn(weights=weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Функция для выполнения детекции и фильтрации только людей\n",
    "def detect_people_retinanet(image, model, device):\n",
    "    img = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "    results = outputs[0]\n",
    "    people = []\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if label == 1 and score > 0.5:  # Assuming 'person' label is 1\n",
    "            people.append({'score': score.item(), 'box': box.tolist(), 'label': 'person'})\n",
    "    return people\n",
    "\n",
    "# Функция для рисования рамок на изображении\n",
    "def draw_boxes(image, results):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "    for res in results:\n",
    "        xmin, ymin, xmax, ymax = res['box']\n",
    "        confidence = res['score']\n",
    "        label = res['label']\n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)\n",
    "        label_with_conf = f\"{label} {confidence:.2f}\"\n",
    "        draw.text((xmin, ymin), label_with_conf, fill='white', font=font)\n",
    "    return image\n",
    "\n",
    "# Функция для преобразования кадра в изображение PIL\n",
    "def frame_to_image(frame):\n",
    "    return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Функция для преобразования изображения PIL обратно в кадр\n",
    "def image_to_frame(image):\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Функция для обработки кадра\n",
    "def process_frame_retinanet(frame, model, device):\n",
    "    image = frame_to_image(frame)\n",
    "    results = detect_people_retinanet(image, model, device)\n",
    "    image_with_boxes = draw_boxes(image, results)\n",
    "    output_frame = image_to_frame(image_with_boxes)\n",
    "    return output_frame\n",
    "\n",
    "def process_video_multithreaded_retinanet(video_path, output_path, model, device, max_workers=2):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        with tqdm(total=total_frames, desc=\"Обработка всего видео (многопоточность)\") as pbar:\n",
    "            for i in range(total_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                if i % 2 == 0:\n",
    "                    futures[executor.submit(process_frame_retinanet, frame, model, device)] = i\n",
    "                else:\n",
    "                    out.write(frame)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                output_frame = future.result()\n",
    "                out.write(output_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# Пути к видеофайлам\n",
    "video_path = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer.mp4'\n",
    "output_path_retinanet = 'C:/Users/Lenovo/Downloads/Caggleton_9/kaggleton_9_longer_retinanet_multithreaded.mp4'\n",
    "\n",
    "# Обработка видео с многопоточностью (RetinaNet)\n",
    "process_video_multithreaded_retinanet(video_path, output_path_retinanet, model, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
